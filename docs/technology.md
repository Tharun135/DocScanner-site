# Technology behind DocScanner

DocScanner AI combines traditional rule-based systems with modern AI to help technical writers efficiently review content. It now runs entirely on your machine, ensuring privacy, speed, and cost-efficiency while staying focused on writing quality.

## Methodology

* Rule-Based Configuration

* Enforces style, grammar, and terminology rules.

* Allows customization according to your organizationâ€™s standards.

### RAG + LLM

Context-aware AI suggestions using Retrieval-Augmented Generation.

Provides suggestions grounded in your existing documentation.

Powered by a local AI stack for secure, cost-free, and fast inference.

## What Powers DocScanner AI

âœ… Local LLM Processing â€“ Runs TinyLLaMA locally via Ollama.

ðŸ”’ Complete Privacy â€“ No data leaves your machine.

âš¡ Fast Responses â€“ Local inference eliminates latency.

ðŸ’° Zero Costs â€“ No recurring API fees.

ðŸ“š Smart Context â€“ Vector search with ChromaDB retrieves the right rules and documentation.

ðŸŽ¯ Writing Focus â€“ Optimized specifically for DocScanner rules and technical writing.

## Tech Stack

* LLM Runtime: Ollama + TinyLLaMA

* Vector Storage: ChromaDB

* RAG Framework: LlamaIndex

* Backend: Python, Flask

* NLP & Rules: spaCy, Custom Rule Engine

* Frontend / Docs: MkDocs with Material theme

### Workflow diagram

```mermaid
graph TD
    A[User Docs] --> B[Rule Engine]
    B --> C[LLM + RAG Suggestions]
    C --> D[Feedback / Export]
```
